{
    "sourceFile": "Chunking/function_app.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1729172894256,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1729172977185,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,222 @@\n+import azure.functions as func\r\n+import logging\r\n+import os\r\n+import json\r\n+import redis\r\n+import spacy\r\n+import fitz # PyMuPDF\r\n+from time import time\r\n+from concurrent.futures import ThreadPoolExecutor\r\n+\r\n+# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n+REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n+REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n+REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n+\r\n+# Configurazione del logger di Windows Server\r\n+def configure_logger():\r\n+    logger = logging.getLogger(\"Chunking\")\r\n+    if not logger.handlers:  # Evita duplicati\r\n+        logger.setLevel(logging.INFO)\r\n+\r\n+        log_directory = './scraping_logs'\r\n+        os.makedirs(log_directory, exist_ok=True)\r\n+\r\n+        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n+        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n+\r\n+        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n+        info_handler.setFormatter(formatter)\r\n+        error_handler.setFormatter(formatter)\r\n+\r\n+        info_handler.setLevel(logging.INFO)\r\n+        error_handler.setLevel(logging.ERROR)\r\n+\r\n+        console_handler = logging.StreamHandler()\r\n+        console_handler.setFormatter(formatter)\r\n+\r\n+        logger.addHandler(info_handler)\r\n+        logger.addHandler(error_handler)\r\n+        logger.addHandler(console_handler)\r\n+\r\n+    return logger\r\n+\r\n+app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n+\r\n+# Inizializzazione del logger\r\n+logger = configure_logger()\r\n+\r\n+# Caricamento del sentencizer per l'italiano\r\n+nlp_it = spacy.blank('it')\r\n+nlp_it.add_pipe(\"sentencizer\")\r\n+nlp_it.max_length = 2000000\r\n+# Caricamento del sentencizer per l'inglese\r\n+nlp_en = spacy.blank('en')\r\n+nlp_en.add_pipe(\"sentencizer\")\r\n+nlp_en.max_length = 2000000\r\n+\r\n+def extract_sentences_from_text(text, nlp_model):\r\n+    \"\"\"Estrazione delle frasi da un blocco di testo usando spaCy.\"\"\"\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il sentencizer spaCy\r\n+    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n+\r\n+def create_text_units(sentences, unit_size=3):\r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n+            units.append(unit)\r\n+            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n+    \r\n+    return units\r\n+\r\n+def extract_text_from_pdf(pdf_path):\r\n+    \"\"\"Estrazione del testo da un PDF usando PyMuPDF.\"\"\"\r\n+    try:\r\n+        #Apertura del PDF con PyMuPDF\r\n+        with fitz.open(pdf_path) as pdf_document:\r\n+            text = \"\"\r\n+            for page_num in range(len(pdf_document)):\r\n+                page = pdf_document.load_page(page_num) # Caricamento della pagina corrente\r\n+                text += page.get_text()  # Estrazione del testo\r\n+        return text\r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nell'estrazione del testo dal PDF {pdf_path}: {e}\")\r\n+        return \"\"\r\n+        \r\n+def process_single_pdf(pdf_path, nlp_model):\r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n+    logger.info(f\"Estrazione del contenuto dal PDF: {os.path.basename(pdf_path)}\")\r\n+    \r\n+    # Estrazione del testo dal PDF e restituzione di tutte le unità\r\n+    text = extract_text_from_pdf(pdf_path)\r\n+    \r\n+    if not text:\r\n+        logger.error(f\"Nessun testo trovato nel PDF {pdf_path}\")\r\n+        return[]\r\n+    \r\n+    # Segmentazione del testo in frasi con spaCy\r\n+    sentences = extract_sentences_from_text(text, nlp_model)\r\n+    \r\n+    # Crazione delle unità di testo\r\n+    all_units = create_text_units(sentences)\r\n+    \r\n+    return all_units\r\n+\r\n+def process_pdf_parallel(pdf_path, client, nlp_model, logger):\r\n+    '''Elaborazione di un singolo PDF, segmentazione del testo in frasi, salvataggio del risultato su Redis.'''\r\n+    try:\r\n+        units = process_single_pdf(pdf_path, nlp_model) # Estrazione delle frasi    \r\n+        client.set(pdf_path, str(units)) # Caching dei risultati in Redis\r\n+        return f\"Elaborazione completata per il PDF: {pdf_path}\", units\r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nell'elaborazione del PDF {pdf_path}: {e}\")\r\n+        return f\"Errore per {pdf_path}: {e}\", []\r\n+\r\n+def save_all_units_to_single_json(all_units, output_filename):\r\n+    \"\"\"Salva tutte le unità estratte da più PDF in un singolo file JSON.\"\"\"\r\n+    try:\r\n+        # Creazione della directory per il file JSON, se non esiste\r\n+        os.makedirs('./documentation_units', exist_ok=True)\r\n+        \r\n+        # Path del file JSON\r\n+        json_filepath = os.path.join('./documentation_units', output_filename)\r\n+        \r\n+        # Struttura delle unità con numero e testo\r\n+        unit_data = [{\"numero_unità\": i + 1, \"testo_unità\": unit} for i, unit in enumerate(all_units)]\r\n+        \r\n+        # Salvataggio delle unità in un file JSON\r\n+        with open(json_filepath, 'w', encoding='utf-8') as json_file:\r\n+            json.dump(unit_data, json_file, ensure_ascii=False, indent=4)\r\n+        \r\n+        logger.info(f\"File JSON unico salvato: {json_filepath}\")\r\n+    \r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nel salvataggio del file JSON unico: {e}\")\r\n+\r\n+def process_documents(pdf_files, client, nlp_model, logger, doc_name):\r\n+    logger.info(f\"Avvio dell'elaborazione della documentazione di {doc_name} ...\")\r\n+    start_time = time()  # Inizio del timer\r\n+\r\n+    messages = []  # Per memorizzare i messaggi di elaborazione\r\n+    all_units = []  # Per memorizzare tutte le unità generate dai PDF\r\n+\r\n+    with ThreadPoolExecutor() as executor:\r\n+        results = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_model, logger), pdf_files))\r\n+\r\n+    for result, units in results:\r\n+        messages.append(result)\r\n+        all_units.extend(units)\r\n+\r\n+    # Salvataggio di tutte le unità in un unico file JSON\r\n+    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}')\r\n+\r\n+    end_time = time()  # Fine del timer\r\n+    total_time = end_time - start_time\r\n+\r\n+    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di {doc_name}: {total_time} secondi\")\r\n+    logger.info(f\"Fine dell'elaborazione della documentazione di {doc_name}\")\r\n+\r\n+@app.route(route=\"http_trigger_chunking\")\r\n+def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n+    logger.info('Python HTTP trigger function processed a request.')\r\n+    \r\n+    # Connessione a Redis\r\n+    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n+    \r\n+    # Pulizia della cache di Redis\r\n+    client.flushdb() # Pulizia del database Redis\r\n+    \r\n+    # Documentazione di Red Hat 8\r\n+    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n+    # Documentazione di Red Hat 9\r\n+    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n+    # Documentazione di Windows Server\r\n+    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n+    \r\n+    # Lista dei PDF Red Hat 8\r\n+    pdf_relh8_files = []\r\n+    # Lista dei PDF Red Hat 9\r\n+    pdf_relh9_files = []\r\n+    # Lista dei PDF Windows Server\r\n+    pdf_ws_files = []\r\n+    \r\n+    # Scansione della documentazione di Red Hat 8\r\n+    for filename in os.listdir(directory_relh8_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh8_files.append(os.path.join(directory_relh8_path, filename))\r\n+    \r\n+    # Scansione della documentazione di Red Hat 9\r\n+    for filename in os.listdir(directory_relh9_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh9_files.append(os.path.join(directory_relh9_path, filename))\r\n+    \r\n+    # Scansione della documentazione di Windows Server\r\n+    for filename in os.listdir(directory_ws_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n+    \r\n+    # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n+    process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n+\r\n+    # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n+    process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n+\r\n+    # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n+    process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n+    \r\n+    # Test della connessione\r\n+    try:\r\n+        \r\n+        if client.ping():\r\n+            logger.info(\"Connessione a Redis riuscita!\")\r\n+            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n+        \r\n+    except Exception as e:\r\n+        \r\n+        logger.error(f\"Errore di connessione: {e}\")\r\n+        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729172994409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,9 +203,9 @@\n     # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n     process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n-    process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n+    #process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n     process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n     \r\n@@ -218,290 +218,5 @@\n         \r\n     except Exception as e:\r\n         \r\n         logger.error(f\"Errore di connessione: {e}\")\r\n-        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n-import azure.functions as func\r\n-import logging\r\n-import os\r\n-import json\r\n-import redis\r\n-import spacy\r\n-import fitz # PyMuPDF\r\n-from time import time\r\n-from concurrent.futures import ThreadPoolExecutor\r\n-\r\n-# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n-REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n-REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n-REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n-\r\n-# Configurazione del logger di Windows Server\r\n-def configure_logger():\r\n-    logger = logging.getLogger(\"Chunking\")\r\n-    if not logger.handlers:  # Evita duplicati\r\n-        logger.setLevel(logging.INFO)\r\n-\r\n-        log_directory = './scraping_logs'\r\n-        os.makedirs(log_directory, exist_ok=True)\r\n-\r\n-        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n-        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n-\r\n-        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n-        info_handler.setFormatter(formatter)\r\n-        error_handler.setFormatter(formatter)\r\n-\r\n-        info_handler.setLevel(logging.INFO)\r\n-        error_handler.setLevel(logging.ERROR)\r\n-\r\n-        console_handler = logging.StreamHandler()\r\n-        console_handler.setFormatter(formatter)\r\n-\r\n-        logger.addHandler(info_handler)\r\n-        logger.addHandler(error_handler)\r\n-        logger.addHandler(console_handler)\r\n-\r\n-    return logger\r\n-\r\n-app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n-\r\n-# Inizializzazione del logger\r\n-logger = configure_logger()\r\n-\r\n-# Caricamento del sentencizer per l'italiano\r\n-nlp_it = spacy.blank('it')\r\n-nlp_it.add_pipe(\"sentencizer\")\r\n-nlp_it.max_length = 2000000\r\n-# Caricamento del sentencizer per l'inglese\r\n-nlp_en = spacy.blank('en')\r\n-nlp_en.add_pipe(\"sentencizer\")\r\n-nlp_en.max_length = 2000000\r\n-\r\n-def extract_sentences_from_text(text, nlp_model):\r\n-    \"\"\"Estrazione delle frasi da un blocco di testo usando spaCy.\"\"\"\r\n-    doc = nlp_model(text)   # Elaborazione del testo con il sentencizer spaCy\r\n-    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n-\r\n-def create_text_units(sentences, unit_size=3):\r\n-    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n-    units = []\r\n-    buffer = []\r\n-    \r\n-    for sentence in sentences:\r\n-        buffer.append(sentence)\r\n-        if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n-            units.append(unit)\r\n-            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n-    \r\n-    return units\r\n-\r\n-def extract_text_from_pdf(pdf_path):\r\n-    \"\"\"Estrazione del testo da un PDF usando PyMuPDF.\"\"\"\r\n-    try:\r\n-        #Apertura del PDF con PyMuPDF\r\n-        with fitz.open(pdf_path) as pdf_document:\r\n-            text = \"\"\r\n-            for page_num in range(len(pdf_document)):\r\n-                page = pdf_document.load_page(page_num) # Caricamento della pagina corrente\r\n-                text += page.get_text()  # Estrazione del testo\r\n-        return text\r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nell'estrazione del testo dal PDF {pdf_path}: {e}\")\r\n-        return \"\"\r\n-        \r\n-def process_single_pdf(pdf_path, nlp_model):\r\n-    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n-    logger.info(f\"Estrazione del contenuto dal PDF: {os.path.basename(pdf_path)}\")\r\n-    \r\n-    # Estrazione del testo dal PDF e restituzione di tutte le unità\r\n-    text = extract_text_from_pdf(pdf_path)\r\n-    \r\n-    if not text:\r\n-        logger.error(f\"Nessun testo trovato nel PDF {pdf_path}\")\r\n-        return[]\r\n-    \r\n-    # Segmentazione del testo in frasi con spaCy\r\n-    sentences = extract_sentences_from_text(text, nlp_model)\r\n-    \r\n-    # Crazione delle unità di testo\r\n-    all_units = create_text_units(sentences)\r\n-    \r\n-    return all_units\r\n-\r\n-def process_pdf_parallel(pdf_path, client, nlp_model, logger):\r\n-    '''Elaborazione di un singolo PDF, segmentazione del testo in frasi, salvataggio del risultato su Redis.'''\r\n-    try:\r\n-        units = process_single_pdf(pdf_path, nlp_model) # Estrazione delle frasi    \r\n-        client.set(pdf_path, str(units)) # Caching dei risultati in Redis\r\n-        return f\"Elaborazione completata per il PDF: {pdf_path}\", units\r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nell'elaborazione del PDF {pdf_path}: {e}\")\r\n-        return f\"Errore per {pdf_path}: {e}\", []\r\n-\r\n-def save_all_units_to_single_json(all_units, output_filename):\r\n-    \"\"\"Salva tutte le unità estratte da più PDF in un singolo file JSON.\"\"\"\r\n-    try:\r\n-        # Creazione della directory per il file JSON, se non esiste\r\n-        os.makedirs('./documentation_units', exist_ok=True)\r\n-        \r\n-        # Path del file JSON\r\n-        json_filepath = os.path.join('./documentation_units', output_filename)\r\n-        \r\n-        # Struttura delle unità con numero e testo\r\n-        unit_data = [{\"numero_unità\": i + 1, \"testo_unità\": unit} for i, unit in enumerate(all_units)]\r\n-        \r\n-        # Salvataggio delle unità in un file JSON\r\n-        with open(json_filepath, 'w', encoding='utf-8') as json_file:\r\n-            json.dump(unit_data, json_file, ensure_ascii=False, indent=4)\r\n-        \r\n-        logger.info(f\"File JSON unico salvato: {json_filepath}\")\r\n-    \r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nel salvataggio del file JSON unico: {e}\")\r\n-\r\n-def process_documents(pdf_files, client, nlp_en, logger, doc_name):\r\n-    logger.info(f\"Avvio dell'elaborazione della documentazione di {doc_name} ...\")\r\n-    start_time = time()  # Inizio del timer\r\n-\r\n-    messages = []  # Per memorizzare i messaggi di elaborazione\r\n-    all_units = []  # Per memorizzare tutte le unità generate dai PDF\r\n-\r\n-    with ThreadPoolExecutor() as executor:\r\n-        results = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_files))\r\n-\r\n-    for result, units in results:\r\n-        messages.append(result)\r\n-        all_units.extend(units)\r\n-\r\n-    # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}')\r\n-\r\n-    end_time = time()  # Fine del timer\r\n-    total_time = end_time - start_time\r\n-\r\n-    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di {doc_name}: {total_time} secondi\")\r\n-    logger.info(f\"Fine dell'elaborazione della documentazione di {doc_name}\")\r\n-\r\n-@app.route(route=\"http_trigger_chunking\")\r\n-def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n-    logger.info('Python HTTP trigger function processed a request.')\r\n-    \r\n-    # Connessione a Redis\r\n-    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n-    \r\n-    # Pulizia della cache di Redis\r\n-    client.flushdb() # Pulizia del database Redis\r\n-    \r\n-    # Documentazione di Red Hat 8\r\n-    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n-    # Documentazione di Red Hat 9\r\n-    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n-    # Documentazione di Windows Server\r\n-    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n-    \r\n-    # Lista dei PDF Red Hat 8\r\n-    pdf_relh8_files = []\r\n-    # Lista dei PDF Red Hat 9\r\n-    pdf_relh9_files = []\r\n-    # Lista dei PDF Windows Server\r\n-    pdf_ws_files = []\r\n-    \r\n-    # Scansione della documentazione di Red Hat 8\r\n-    for filename in os.listdir(directory_relh8_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh8_files.append(os.path.join(directory_relh8_path, filename))\r\n-    \r\n-    # Scansione della documentazione di Red Hat 9\r\n-    for filename in os.listdir(directory_relh9_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh9_files.append(os.path.join(directory_relh9_path, filename))\r\n-    \r\n-    # Scansione della documentazione di Windows Server\r\n-    for filename in os.listdir(directory_ws_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n-    \r\n-    # Elaborazione di ciascun PDF della documentazione di Red Hat 8 in parallelo\r\n-    logger.info(\"Avvio dell'elaborazione della documentazione di Red Hat 8 ...\")\r\n-    \r\n-    start_time_relh8 = time() # Inizio del timer per l'elaborazione della documentazione Red Hat 8\r\n-    \r\n-    messages_relh8 = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n-    all_units_relh8 = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n-    \r\n-    with ThreadPoolExecutor() as executor:\r\n-        results_relh8 = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_relh8_files))\r\n-    \r\n-    for result_relh8, units_relh8 in results_relh8:\r\n-        messages_relh8.append(result_relh8)\r\n-        all_units_relh8.extend(units_relh8)\r\n-    \r\n-    # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units_relh8, 'all_units_relh8')\r\n-    \r\n-    end_time_relh8 = time() # Fine del timer per l'elaborazione della documentazione Red Hat 8\r\n-    total_time_relh8 = end_time_relh8 - start_time_relh8\r\n-    \r\n-    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Red Hat 8: {total_time_relh8} secondi\")\r\n-    logger.info(\"Fine dell'elaborazione della documentazione di Red Hat 8\")\r\n-    \r\n-    # Elaborazione di ciascun PDF della documentazione di Red Hat 9 in parallelo\r\n-    logger.info(\"Avvio dell'elaborazione della documentazione di Red Hat 9 ...\")\r\n-    \r\n-    start_time_relh9 = time() # Inizio del timer per l'elaborazione della documentazione Red Hat 9\r\n-    \r\n-    messages_relh9 = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n-    all_units_relh9 = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n-    \r\n-    with ThreadPoolExecutor() as executor:\r\n-        results_relh9 = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_relh9_files))\r\n-    \r\n-    for result_relh9, units_relh9 in results_relh9:\r\n-        messages_relh9.append(result_relh9)\r\n-        all_units_relh9.extend(units_relh9)\r\n-    \r\n-    # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units_relh9, 'all_units_relh9')\r\n-    \r\n-    end_time_relh9 = time() # Fine del timer per l'elaborazione della documentazione Red Hat 9\r\n-    total_time_relh9 = end_time_relh9 - start_time_relh9\r\n-    \r\n-    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Red Hat 9: {total_time_relh9} secondi\")\r\n-    logger.info(\"Fine dell'elaborazione della documentazione di Red Hat 9\")\r\n-    \r\n-    # Elaborazione di ciascun PDF della documentazione di Windows Server in parallelo\r\n-    logger.info(\"Avvio dell'elaborazione della documentazione di Windows Server ...\")\r\n-    \r\n-    start_time_ws = time() # Inizio del timer per l'elaborazione della documentazione Windows Server\r\n-    \r\n-    messages_ws = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n-    all_units_ws = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n-    \r\n-    with ThreadPoolExecutor() as executor:\r\n-        results_ws = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_ws_files))\r\n-    \r\n-    for result_ws, units_ws in results_ws:\r\n-        messages_ws.append(result_ws)\r\n-        all_units_ws.extend(units_ws)\r\n-    \r\n-    # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units_ws, 'all_units_ws')\r\n-    \r\n-    end_time_ws = time() # Fine del timer per l'elaborazione della documentazione Windows Server\r\n-    total_time_ws = end_time_ws - start_time_ws\r\n-    \r\n-    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Windows Server: {total_time_ws} secondi\")\r\n-    logger.info(\"Fine dell'elaborazione della documentazione di Windows Server ...\")\r\n-    \r\n-    # Test della connessione\r\n-    try:\r\n-        \r\n-        if client.ping():\r\n-            logger.info(\"Connessione a Redis riuscita!\")\r\n-            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n-        \r\n-    except Exception as e:\r\n-        \r\n-        logger.error(f\"Errore di connessione: {e}\")\r\n         return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729173002684,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,12 +203,12 @@\n     # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n     process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n-    #process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n+    # process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n-    process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n+    # process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n     \r\n     # Test della connessione\r\n     try:\r\n         \r\n"
                },
                {
                    "date": 1729173312138,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,9 +152,9 @@\n         messages.append(result)\r\n         all_units.extend(units)\r\n \r\n     # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}')\r\n+    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}.json')\r\n \r\n     end_time = time()  # Fine del timer\r\n     total_time = end_time - start_time\r\n \r\n"
                },
                {
                    "date": 1729173332280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,222 @@\n+import azure.functions as func\r\n+import logging\r\n+import os\r\n+import json\r\n+import redis\r\n+import spacy\r\n+import fitz # PyMuPDF\r\n+from time import time\r\n+from concurrent.futures import ThreadPoolExecutor\r\n+\r\n+# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n+REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n+REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n+REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n+\r\n+# Configurazione del logger di Windows Server\r\n+def configure_logger():\r\n+    logger = logging.getLogger(\"Chunking\")\r\n+    if not logger.handlers:  # Evita duplicati\r\n+        logger.setLevel(logging.INFO)\r\n+\r\n+        log_directory = './scraping_logs'\r\n+        os.makedirs(log_directory, exist_ok=True)\r\n+\r\n+        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n+        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n+\r\n+        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n+        info_handler.setFormatter(formatter)\r\n+        error_handler.setFormatter(formatter)\r\n+\r\n+        info_handler.setLevel(logging.INFO)\r\n+        error_handler.setLevel(logging.ERROR)\r\n+\r\n+        console_handler = logging.StreamHandler()\r\n+        console_handler.setFormatter(formatter)\r\n+\r\n+        logger.addHandler(info_handler)\r\n+        logger.addHandler(error_handler)\r\n+        logger.addHandler(console_handler)\r\n+\r\n+    return logger\r\n+\r\n+app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n+\r\n+# Inizializzazione del logger\r\n+logger = configure_logger()\r\n+\r\n+# Caricamento del sentencizer per l'italiano\r\n+nlp_it = spacy.blank('it')\r\n+nlp_it.add_pipe(\"sentencizer\")\r\n+nlp_it.max_length = 2000000\r\n+# Caricamento del sentencizer per l'inglese\r\n+nlp_en = spacy.blank('en')\r\n+nlp_en.add_pipe(\"sentencizer\")\r\n+nlp_en.max_length = 2000000\r\n+\r\n+def extract_sentences_from_text(text, nlp_model):\r\n+    \"\"\"Estrazione delle frasi da un blocco di testo usando spaCy.\"\"\"\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il sentencizer spaCy\r\n+    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n+\r\n+def create_text_units(sentences, unit_size=3):\r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n+            units.append(unit)\r\n+            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n+    \r\n+    return units\r\n+\r\n+def extract_text_from_pdf(pdf_path):\r\n+    \"\"\"Estrazione del testo da un PDF usando PyMuPDF.\"\"\"\r\n+    try:\r\n+        #Apertura del PDF con PyMuPDF\r\n+        with fitz.open(pdf_path) as pdf_document:\r\n+            text = \"\"\r\n+            for page_num in range(len(pdf_document)):\r\n+                page = pdf_document.load_page(page_num) # Caricamento della pagina corrente\r\n+                text += page.get_text()  # Estrazione del testo\r\n+        return text\r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nell'estrazione del testo dal PDF {pdf_path}: {e}\")\r\n+        return \"\"\r\n+        \r\n+def process_single_pdf(pdf_path, nlp_model):\r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n+    logger.info(f\"Estrazione del contenuto dal PDF: {os.path.basename(pdf_path)}\")\r\n+    \r\n+    # Estrazione del testo dal PDF e restituzione di tutte le unità\r\n+    text = extract_text_from_pdf(pdf_path)\r\n+    \r\n+    if not text:\r\n+        logger.error(f\"Nessun testo trovato nel PDF {pdf_path}\")\r\n+        return[]\r\n+    \r\n+    # Segmentazione del testo in frasi con spaCy\r\n+    sentences = extract_sentences_from_text(text, nlp_model)\r\n+    \r\n+    # Crazione delle unità di testo\r\n+    all_units = create_text_units(sentences)\r\n+    \r\n+    return all_units\r\n+\r\n+def process_pdf_parallel(pdf_path, client, nlp_model, logger):\r\n+    '''Elaborazione di un singolo PDF, segmentazione del testo in frasi, salvataggio del risultato su Redis.'''\r\n+    try:\r\n+        units = process_single_pdf(pdf_path, nlp_model) # Estrazione delle frasi    \r\n+        client.set(pdf_path, str(units)) # Caching dei risultati in Redis\r\n+        return f\"Elaborazione completata per il PDF: {pdf_path}\", units\r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nell'elaborazione del PDF {pdf_path}: {e}\")\r\n+        return f\"Errore per {pdf_path}: {e}\", []\r\n+\r\n+def save_all_units_to_single_json(all_units, output_filename):\r\n+    \"\"\"Salva tutte le unità estratte da più PDF in un singolo file JSON.\"\"\"\r\n+    try:\r\n+        # Creazione della directory per il file JSON, se non esiste\r\n+        os.makedirs('./documentation_units', exist_ok=True)\r\n+        \r\n+        # Path del file JSON\r\n+        json_filepath = os.path.join('./documentation_units', output_filename)\r\n+        \r\n+        # Struttura delle unità con numero e testo\r\n+        unit_data = [{\"numero_unità\": i + 1, \"testo_unità\": unit} for i, unit in enumerate(all_units)]\r\n+        \r\n+        # Salvataggio delle unità in un file JSON\r\n+        with open(json_filepath, 'w', encoding='utf-8') as json_file:\r\n+            json.dump(unit_data, json_file, ensure_ascii=False, indent=4)\r\n+        \r\n+        logger.info(f\"File JSON unico salvato: {json_filepath}\")\r\n+    \r\n+    except Exception as e:\r\n+        logger.error(f\"Errore nel salvataggio del file JSON unico: {e}\")\r\n+\r\n+def process_documents(pdf_files, client, nlp_model, logger, doc_name):\r\n+    logger.info(f\"Avvio dell'elaborazione della documentazione di {doc_name} ...\")\r\n+    start_time = time()  # Inizio del timer\r\n+\r\n+    messages = []  # Per memorizzare i messaggi di elaborazione\r\n+    all_units = []  # Per memorizzare tutte le unità generate dai PDF\r\n+\r\n+    with ThreadPoolExecutor() as executor:\r\n+        results = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_model, logger), pdf_files))\r\n+\r\n+    for result, units in results:\r\n+        messages.append(result)\r\n+        all_units.extend(units)\r\n+\r\n+    # Salvataggio di tutte le unità in un unico file JSON\r\n+    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}.json')\r\n+\r\n+    end_time = time()  # Fine del timer\r\n+    total_time = end_time - start_time\r\n+\r\n+    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di {doc_name}: {total_time} secondi\")\r\n+    logger.info(f\"Fine dell'elaborazione della documentazione di {doc_name}\")\r\n+\r\n+@app.route(route=\"http_trigger_chunking\")\r\n+def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n+    logger.info('Python HTTP trigger function processed a request.')\r\n+    \r\n+    # Connessione a Redis\r\n+    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n+    \r\n+    # Pulizia della cache di Redis\r\n+    client.flushdb() # Pulizia del database Redis\r\n+    \r\n+    # Documentazione di Red Hat 8\r\n+    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n+    # Documentazione di Red Hat 9\r\n+    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n+    # Documentazione di Windows Server\r\n+    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n+    \r\n+    # Lista dei PDF Red Hat 8\r\n+    pdf_relh8_files = []\r\n+    # Lista dei PDF Red Hat 9\r\n+    pdf_relh9_files = []\r\n+    # Lista dei PDF Windows Server\r\n+    pdf_ws_files = []\r\n+    \r\n+    # Scansione della documentazione di Red Hat 8\r\n+    for filename in os.listdir(directory_relh8_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh8_files.append(os.path.join(directory_relh8_path, filename))\r\n+    \r\n+    # Scansione della documentazione di Red Hat 9\r\n+    for filename in os.listdir(directory_relh9_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh9_files.append(os.path.join(directory_relh9_path, filename))\r\n+    \r\n+    # Scansione della documentazione di Windows Server\r\n+    for filename in os.listdir(directory_ws_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n+    \r\n+    # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n+    # process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n+\r\n+    # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n+    process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n+\r\n+    # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n+    # process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n+    \r\n+    # Test della connessione\r\n+    try:\r\n+        \r\n+        if client.ping():\r\n+            logger.info(\"Connessione a Redis riuscita!\")\r\n+            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n+        \r\n+    except Exception as e:\r\n+        \r\n+        logger.error(f\"Errore di connessione: {e}\")\r\n+        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729173434375,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,234 +203,12 @@\n     # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n     # process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n-    process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n-\r\n-    # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n-    # process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n-    \r\n-    # Test della connessione\r\n-    try:\r\n-        \r\n-        if client.ping():\r\n-            logger.info(\"Connessione a Redis riuscita!\")\r\n-            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n-        \r\n-    except Exception as e:\r\n-        \r\n-        logger.error(f\"Errore di connessione: {e}\")\r\n-        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n-import azure.functions as func\r\n-import logging\r\n-import os\r\n-import json\r\n-import redis\r\n-import spacy\r\n-import fitz # PyMuPDF\r\n-from time import time\r\n-from concurrent.futures import ThreadPoolExecutor\r\n-\r\n-# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n-REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n-REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n-REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n-\r\n-# Configurazione del logger di Windows Server\r\n-def configure_logger():\r\n-    logger = logging.getLogger(\"Chunking\")\r\n-    if not logger.handlers:  # Evita duplicati\r\n-        logger.setLevel(logging.INFO)\r\n-\r\n-        log_directory = './scraping_logs'\r\n-        os.makedirs(log_directory, exist_ok=True)\r\n-\r\n-        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n-        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n-\r\n-        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n-        info_handler.setFormatter(formatter)\r\n-        error_handler.setFormatter(formatter)\r\n-\r\n-        info_handler.setLevel(logging.INFO)\r\n-        error_handler.setLevel(logging.ERROR)\r\n-\r\n-        console_handler = logging.StreamHandler()\r\n-        console_handler.setFormatter(formatter)\r\n-\r\n-        logger.addHandler(info_handler)\r\n-        logger.addHandler(error_handler)\r\n-        logger.addHandler(console_handler)\r\n-\r\n-    return logger\r\n-\r\n-app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n-\r\n-# Inizializzazione del logger\r\n-logger = configure_logger()\r\n-\r\n-# Caricamento del sentencizer per l'italiano\r\n-nlp_it = spacy.blank('it')\r\n-nlp_it.add_pipe(\"sentencizer\")\r\n-nlp_it.max_length = 2000000\r\n-# Caricamento del sentencizer per l'inglese\r\n-nlp_en = spacy.blank('en')\r\n-nlp_en.add_pipe(\"sentencizer\")\r\n-nlp_en.max_length = 2000000\r\n-\r\n-def extract_sentences_from_text(text, nlp_model):\r\n-    \"\"\"Estrazione delle frasi da un blocco di testo usando spaCy.\"\"\"\r\n-    doc = nlp_model(text)   # Elaborazione del testo con il sentencizer spaCy\r\n-    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n-\r\n-def create_text_units(sentences, unit_size=3):\r\n-    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n-    units = []\r\n-    buffer = []\r\n-    \r\n-    for sentence in sentences:\r\n-        buffer.append(sentence)\r\n-        if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n-            units.append(unit)\r\n-            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n-    \r\n-    return units\r\n-\r\n-def extract_text_from_pdf(pdf_path):\r\n-    \"\"\"Estrazione del testo da un PDF usando PyMuPDF.\"\"\"\r\n-    try:\r\n-        #Apertura del PDF con PyMuPDF\r\n-        with fitz.open(pdf_path) as pdf_document:\r\n-            text = \"\"\r\n-            for page_num in range(len(pdf_document)):\r\n-                page = pdf_document.load_page(page_num) # Caricamento della pagina corrente\r\n-                text += page.get_text()  # Estrazione del testo\r\n-        return text\r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nell'estrazione del testo dal PDF {pdf_path}: {e}\")\r\n-        return \"\"\r\n-        \r\n-def process_single_pdf(pdf_path, nlp_model):\r\n-    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n-    logger.info(f\"Estrazione del contenuto dal PDF: {os.path.basename(pdf_path)}\")\r\n-    \r\n-    # Estrazione del testo dal PDF e restituzione di tutte le unità\r\n-    text = extract_text_from_pdf(pdf_path)\r\n-    \r\n-    if not text:\r\n-        logger.error(f\"Nessun testo trovato nel PDF {pdf_path}\")\r\n-        return[]\r\n-    \r\n-    # Segmentazione del testo in frasi con spaCy\r\n-    sentences = extract_sentences_from_text(text, nlp_model)\r\n-    \r\n-    # Crazione delle unità di testo\r\n-    all_units = create_text_units(sentences)\r\n-    \r\n-    return all_units\r\n-\r\n-def process_pdf_parallel(pdf_path, client, nlp_model, logger):\r\n-    '''Elaborazione di un singolo PDF, segmentazione del testo in frasi, salvataggio del risultato su Redis.'''\r\n-    try:\r\n-        units = process_single_pdf(pdf_path, nlp_model) # Estrazione delle frasi    \r\n-        client.set(pdf_path, str(units)) # Caching dei risultati in Redis\r\n-        return f\"Elaborazione completata per il PDF: {pdf_path}\", units\r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nell'elaborazione del PDF {pdf_path}: {e}\")\r\n-        return f\"Errore per {pdf_path}: {e}\", []\r\n-\r\n-def save_all_units_to_single_json(all_units, output_filename):\r\n-    \"\"\"Salva tutte le unità estratte da più PDF in un singolo file JSON.\"\"\"\r\n-    try:\r\n-        # Creazione della directory per il file JSON, se non esiste\r\n-        os.makedirs('./documentation_units', exist_ok=True)\r\n-        \r\n-        # Path del file JSON\r\n-        json_filepath = os.path.join('./documentation_units', output_filename)\r\n-        \r\n-        # Struttura delle unità con numero e testo\r\n-        unit_data = [{\"numero_unità\": i + 1, \"testo_unità\": unit} for i, unit in enumerate(all_units)]\r\n-        \r\n-        # Salvataggio delle unità in un file JSON\r\n-        with open(json_filepath, 'w', encoding='utf-8') as json_file:\r\n-            json.dump(unit_data, json_file, ensure_ascii=False, indent=4)\r\n-        \r\n-        logger.info(f\"File JSON unico salvato: {json_filepath}\")\r\n-    \r\n-    except Exception as e:\r\n-        logger.error(f\"Errore nel salvataggio del file JSON unico: {e}\")\r\n-\r\n-def process_documents(pdf_files, client, nlp_model, logger, doc_name):\r\n-    logger.info(f\"Avvio dell'elaborazione della documentazione di {doc_name} ...\")\r\n-    start_time = time()  # Inizio del timer\r\n-\r\n-    messages = []  # Per memorizzare i messaggi di elaborazione\r\n-    all_units = []  # Per memorizzare tutte le unità generate dai PDF\r\n-\r\n-    with ThreadPoolExecutor() as executor:\r\n-        results = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_model, logger), pdf_files))\r\n-\r\n-    for result, units in results:\r\n-        messages.append(result)\r\n-        all_units.extend(units)\r\n-\r\n-    # Salvataggio di tutte le unità in un unico file JSON\r\n-    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}.json')\r\n-\r\n-    end_time = time()  # Fine del timer\r\n-    total_time = end_time - start_time\r\n-\r\n-    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di {doc_name}: {total_time} secondi\")\r\n-    logger.info(f\"Fine dell'elaborazione della documentazione di {doc_name}\")\r\n-\r\n-@app.route(route=\"http_trigger_chunking\")\r\n-def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n-    logger.info('Python HTTP trigger function processed a request.')\r\n-    \r\n-    # Connessione a Redis\r\n-    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n-    \r\n-    # Pulizia della cache di Redis\r\n-    client.flushdb() # Pulizia del database Redis\r\n-    \r\n-    # Documentazione di Red Hat 8\r\n-    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n-    # Documentazione di Red Hat 9\r\n-    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n-    # Documentazione di Windows Server\r\n-    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n-    \r\n-    # Lista dei PDF Red Hat 8\r\n-    pdf_relh8_files = []\r\n-    # Lista dei PDF Red Hat 9\r\n-    pdf_relh9_files = []\r\n-    # Lista dei PDF Windows Server\r\n-    pdf_ws_files = []\r\n-    \r\n-    # Scansione della documentazione di Red Hat 8\r\n-    for filename in os.listdir(directory_relh8_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh8_files.append(os.path.join(directory_relh8_path, filename))\r\n-    \r\n-    # Scansione della documentazione di Red Hat 9\r\n-    for filename in os.listdir(directory_relh9_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh9_files.append(os.path.join(directory_relh9_path, filename))\r\n-    \r\n-    # Scansione della documentazione di Windows Server\r\n-    for filename in os.listdir(directory_ws_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n-    \r\n-    # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n-    process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n-\r\n-    # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n     # process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n-    # process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n+    process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n     \r\n     # Test della connessione\r\n     try:\r\n         \r\n"
                },
                {
                    "date": 1729173499226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -200,12 +200,12 @@\n         if filename.endswith('.pdf'):\r\n             pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n     \r\n     # Elaborazione di ciascun PDF della documentazione Red Hat 8\r\n-    # process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n+    process_documents(pdf_relh8_files, client, nlp_en, logger, \"Red Hat 8\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione Red Hat 9 (solo dopo Red Hat 8)\r\n-    # process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n+    process_documents(pdf_relh9_files, client, nlp_en, logger, \"Red Hat 9\")\r\n \r\n     # Elaborazione di ciascun PDF della documentazione di Windows Server (solo dopo Red Hat 9)\r\n     process_documents(pdf_ws_files, client, nlp_en, logger, \"Windows Server\")\r\n     \r\n"
                }
            ],
            "date": 1729172894256,
            "name": "Commit-0",
            "content": "import azure.functions as func\r\nimport logging\r\nimport os\r\nimport json\r\nimport redis\r\nimport spacy\r\nimport fitz # PyMuPDF\r\nfrom time import time\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\n# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\nREDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\nREDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\nREDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n\r\n# Configurazione del logger di Windows Server\r\ndef configure_logger():\r\n    logger = logging.getLogger(\"Chunking\")\r\n    if not logger.handlers:  # Evita duplicati\r\n        logger.setLevel(logging.INFO)\r\n\r\n        log_directory = './scraping_logs'\r\n        os.makedirs(log_directory, exist_ok=True)\r\n\r\n        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n\r\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n        info_handler.setFormatter(formatter)\r\n        error_handler.setFormatter(formatter)\r\n\r\n        info_handler.setLevel(logging.INFO)\r\n        error_handler.setLevel(logging.ERROR)\r\n\r\n        console_handler = logging.StreamHandler()\r\n        console_handler.setFormatter(formatter)\r\n\r\n        logger.addHandler(info_handler)\r\n        logger.addHandler(error_handler)\r\n        logger.addHandler(console_handler)\r\n\r\n    return logger\r\n\r\napp = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n\r\n# Inizializzazione del logger\r\nlogger = configure_logger()\r\n\r\n# Caricamento del sentencizer per l'italiano\r\nnlp_it = spacy.blank('it')\r\nnlp_it.add_pipe(\"sentencizer\")\r\nnlp_it.max_length = 2000000\r\n# Caricamento del sentencizer per l'inglese\r\nnlp_en = spacy.blank('en')\r\nnlp_en.add_pipe(\"sentencizer\")\r\nnlp_en.max_length = 2000000\r\n\r\ndef extract_sentences_from_text(text, nlp_model):\r\n    \"\"\"Estrazione delle frasi da un blocco di testo usando spaCy.\"\"\"\r\n    doc = nlp_model(text)   # Elaborazione del testo con il sentencizer spaCy\r\n    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n\r\ndef create_text_units(sentences, unit_size=3):\r\n    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n    units = []\r\n    buffer = []\r\n    \r\n    for sentence in sentences:\r\n        buffer.append(sentence)\r\n        if len(buffer) >= unit_size:\r\n            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n            units.append(unit)\r\n            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n    \r\n    return units\r\n\r\ndef extract_text_from_pdf(pdf_path):\r\n    \"\"\"Estrazione del testo da un PDF usando PyMuPDF.\"\"\"\r\n    try:\r\n        #Apertura del PDF con PyMuPDF\r\n        with fitz.open(pdf_path) as pdf_document:\r\n            text = \"\"\r\n            for page_num in range(len(pdf_document)):\r\n                page = pdf_document.load_page(page_num) # Caricamento della pagina corrente\r\n                text += page.get_text()  # Estrazione del testo\r\n        return text\r\n    except Exception as e:\r\n        logger.error(f\"Errore nell'estrazione del testo dal PDF {pdf_path}: {e}\")\r\n        return \"\"\r\n        \r\ndef process_single_pdf(pdf_path, nlp_model):\r\n    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n    logger.info(f\"Estrazione del contenuto dal PDF: {os.path.basename(pdf_path)}\")\r\n    \r\n    # Estrazione del testo dal PDF e restituzione di tutte le unità\r\n    text = extract_text_from_pdf(pdf_path)\r\n    \r\n    if not text:\r\n        logger.error(f\"Nessun testo trovato nel PDF {pdf_path}\")\r\n        return[]\r\n    \r\n    # Segmentazione del testo in frasi con spaCy\r\n    sentences = extract_sentences_from_text(text, nlp_model)\r\n    \r\n    # Crazione delle unità di testo\r\n    all_units = create_text_units(sentences)\r\n    \r\n    return all_units\r\n\r\ndef process_pdf_parallel(pdf_path, client, nlp_model, logger):\r\n    '''Elaborazione di un singolo PDF, segmentazione del testo in frasi, salvataggio del risultato su Redis.'''\r\n    try:\r\n        units = process_single_pdf(pdf_path, nlp_model) # Estrazione delle frasi    \r\n        client.set(pdf_path, str(units)) # Caching dei risultati in Redis\r\n        return f\"Elaborazione completata per il PDF: {pdf_path}\", units\r\n    except Exception as e:\r\n        logger.error(f\"Errore nell'elaborazione del PDF {pdf_path}: {e}\")\r\n        return f\"Errore per {pdf_path}: {e}\", []\r\n\r\ndef save_all_units_to_single_json(all_units, output_filename):\r\n    \"\"\"Salva tutte le unità estratte da più PDF in un singolo file JSON.\"\"\"\r\n    try:\r\n        # Creazione della directory per il file JSON, se non esiste\r\n        os.makedirs('./documentation_units', exist_ok=True)\r\n        \r\n        # Path del file JSON\r\n        json_filepath = os.path.join('./documentation_units', output_filename)\r\n        \r\n        # Struttura delle unità con numero e testo\r\n        unit_data = [{\"numero_unità\": i + 1, \"testo_unità\": unit} for i, unit in enumerate(all_units)]\r\n        \r\n        # Salvataggio delle unità in un file JSON\r\n        with open(json_filepath, 'w', encoding='utf-8') as json_file:\r\n            json.dump(unit_data, json_file, ensure_ascii=False, indent=4)\r\n        \r\n        logger.info(f\"File JSON unico salvato: {json_filepath}\")\r\n    \r\n    except Exception as e:\r\n        logger.error(f\"Errore nel salvataggio del file JSON unico: {e}\")\r\n\r\ndef process_documents(pdf_files, client, nlp_en, logger, doc_name):\r\n    logger.info(f\"Avvio dell'elaborazione della documentazione di {doc_name} ...\")\r\n    start_time = time()  # Inizio del timer\r\n\r\n    messages = []  # Per memorizzare i messaggi di elaborazione\r\n    all_units = []  # Per memorizzare tutte le unità generate dai PDF\r\n\r\n    with ThreadPoolExecutor() as executor:\r\n        results = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_files))\r\n\r\n    for result, units in results:\r\n        messages.append(result)\r\n        all_units.extend(units)\r\n\r\n    # Salvataggio di tutte le unità in un unico file JSON\r\n    save_all_units_to_single_json(all_units, f'all_units_{doc_name.lower()}')\r\n\r\n    end_time = time()  # Fine del timer\r\n    total_time = end_time - start_time\r\n\r\n    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di {doc_name}: {total_time} secondi\")\r\n    logger.info(f\"Fine dell'elaborazione della documentazione di {doc_name}\")\r\n\r\n@app.route(route=\"http_trigger_chunking\")\r\ndef http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n    logger.info('Python HTTP trigger function processed a request.')\r\n    \r\n    # Connessione a Redis\r\n    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n    \r\n    # Pulizia della cache di Redis\r\n    client.flushdb() # Pulizia del database Redis\r\n    \r\n    # Documentazione di Red Hat 8\r\n    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n    # Documentazione di Red Hat 9\r\n    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n    # Documentazione di Windows Server\r\n    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n    \r\n    # Lista dei PDF Red Hat 8\r\n    pdf_relh8_files = []\r\n    # Lista dei PDF Red Hat 9\r\n    pdf_relh9_files = []\r\n    # Lista dei PDF Windows Server\r\n    pdf_ws_files = []\r\n    \r\n    # Scansione della documentazione di Red Hat 8\r\n    for filename in os.listdir(directory_relh8_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_relh8_files.append(os.path.join(directory_relh8_path, filename))\r\n    \r\n    # Scansione della documentazione di Red Hat 9\r\n    for filename in os.listdir(directory_relh9_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_relh9_files.append(os.path.join(directory_relh9_path, filename))\r\n    \r\n    # Scansione della documentazione di Windows Server\r\n    for filename in os.listdir(directory_ws_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_ws_files.append(os.path.join(directory_ws_path, filename))\r\n    \r\n    # Elaborazione di ciascun PDF della documentazione di Red Hat 8 in parallelo\r\n    logger.info(\"Avvio dell'elaborazione della documentazione di Red Hat 8 ...\")\r\n    \r\n    start_time_relh8 = time() # Inizio del timer per l'elaborazione della documentazione Red Hat 8\r\n    \r\n    messages_relh8 = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n    all_units_relh8 = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n    \r\n    with ThreadPoolExecutor() as executor:\r\n        results_relh8 = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_relh8_files))\r\n    \r\n    for result_relh8, units_relh8 in results_relh8:\r\n        messages_relh8.append(result_relh8)\r\n        all_units_relh8.extend(units_relh8)\r\n    \r\n    # Salvataggio di tutte le unità in un unico file JSON\r\n    save_all_units_to_single_json(all_units_relh8, 'all_units_relh8')\r\n    \r\n    end_time_relh8 = time() # Fine del timer per l'elaborazione della documentazione Red Hat 8\r\n    total_time_relh8 = end_time_relh8 - start_time_relh8\r\n    \r\n    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Red Hat 8: {total_time_relh8} secondi\")\r\n    logger.info(\"Fine dell'elaborazione della documentazione di Red Hat 8\")\r\n    \r\n    # Elaborazione di ciascun PDF della documentazione di Red Hat 9 in parallelo\r\n    logger.info(\"Avvio dell'elaborazione della documentazione di Red Hat 9 ...\")\r\n    \r\n    start_time_relh9 = time() # Inizio del timer per l'elaborazione della documentazione Red Hat 9\r\n    \r\n    messages_relh9 = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n    all_units_relh9 = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n    \r\n    with ThreadPoolExecutor() as executor:\r\n        results_relh9 = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_relh9_files))\r\n    \r\n    for result_relh9, units_relh9 in results_relh9:\r\n        messages_relh9.append(result_relh9)\r\n        all_units_relh9.extend(units_relh9)\r\n    \r\n    # Salvataggio di tutte le unità in un unico file JSON\r\n    save_all_units_to_single_json(all_units_relh9, 'all_units_relh9')\r\n    \r\n    end_time_relh9 = time() # Fine del timer per l'elaborazione della documentazione Red Hat 9\r\n    total_time_relh9 = end_time_relh9 - start_time_relh9\r\n    \r\n    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Red Hat 9: {total_time_relh9} secondi\")\r\n    logger.info(\"Fine dell'elaborazione della documentazione di Red Hat 9\")\r\n    \r\n    # Elaborazione di ciascun PDF della documentazione di Windows Server in parallelo\r\n    logger.info(\"Avvio dell'elaborazione della documentazione di Windows Server ...\")\r\n    \r\n    start_time_ws = time() # Inizio del timer per l'elaborazione della documentazione Windows Server\r\n    \r\n    messages_ws = [] # Utilizzato per memorizzare i messaggi di elaborazione\r\n    all_units_ws = [] # Utilizzato per memorizzare tutte le unità generate dai PDF\r\n    \r\n    with ThreadPoolExecutor() as executor:\r\n        results_ws = list(executor.map(lambda pdf: process_pdf_parallel(pdf, client, nlp_en, logger), pdf_ws_files))\r\n    \r\n    for result_ws, units_ws in results_ws:\r\n        messages_ws.append(result_ws)\r\n        all_units_ws.extend(units_ws)\r\n    \r\n    # Salvataggio di tutte le unità in un unico file JSON\r\n    save_all_units_to_single_json(all_units_ws, 'all_units_ws')\r\n    \r\n    end_time_ws = time() # Fine del timer per l'elaborazione della documentazione Windows Server\r\n    total_time_ws = end_time_ws - start_time_ws\r\n    \r\n    logger.info(f\"Tempo totale di esecuzione per l'elaborazione della documentazione di Windows Server: {total_time_ws} secondi\")\r\n    logger.info(\"Fine dell'elaborazione della documentazione di Windows Server ...\")\r\n    \r\n    # Test della connessione\r\n    try:\r\n        \r\n        if client.ping():\r\n            logger.info(\"Connessione a Redis riuscita!\")\r\n            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n        \r\n    except Exception as e:\r\n        \r\n        logger.error(f\"Errore di connessione: {e}\")\r\n        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)"
        }
    ]
}