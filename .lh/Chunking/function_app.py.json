{
    "sourceFile": "Chunking/function_app.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 25,
            "patches": [
                {
                    "date": 1729063782966,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1729063881116,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,9 +56,20 @@\n     doc = nlp_model(text)   # Processamento del testo con il modello spaCy\r\n     return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n \r\n def create_text_units(sentences, unit_size=3):\r\n-    \"\"\"Creazione delle unità di testo costituite da \"\"\"\r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size])\r\n+            units.append(unit)\r\n+            buffer.pop(0)\r\n+    \r\n+    return units\r\n \r\n def process_pdf(pdf_path, nlp_model):\r\n     \"\"\"Processa il PDF e restituisce tutte le frasi.\"\"\"\r\n     document = fitz.open(pdf_path)  # Apri il PDF\r\n"
                },
                {
                    "date": 1729064166751,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,15 +50,19 @@\n nlp_en = spacy.load('en_core_web_sm')\r\n \r\n \r\n def extract_sentences_from_page(page, nlp_model):\r\n+    \r\n     \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n+    \r\n     text = page.get_text()  # Estrazione del testo da una pagina\r\n     doc = nlp_model(text)   # Processamento del testo con il modello spaCy\r\n     return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n \r\n def create_text_units(sentences, unit_size=3):\r\n+    \r\n     \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    \r\n     units = []\r\n     buffer = []\r\n     \r\n     for sentence in sentences:\r\n@@ -70,9 +74,11 @@\n     \r\n     return units\r\n \r\n def process_pdf(pdf_path, nlp_model):\r\n+    \r\n     \"\"\"Processa il PDF e restituisce tutte le frasi.\"\"\"\r\n+    \r\n     document = fitz.open(pdf_path)  # Apri il PDF\r\n     all_sentences = []\r\n \r\n     for page_num in range(document.page_count):  # Itera su ogni pagina del PDF\r\n"
                },
                {
                    "date": 1729064193732,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,9 +75,9 @@\n     return units\r\n \r\n def process_pdf(pdf_path, nlp_model):\r\n     \r\n-    \"\"\"Processa il PDF e restituisce tutte le frasi.\"\"\"\r\n+    \"\"\"Processamento del PDF e restituisce tutte le frasi.\"\"\"\r\n     \r\n     document = fitz.open(pdf_path)  # Apri il PDF\r\n     all_sentences = []\r\n \r\n"
                },
                {
                    "date": 1729064484419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,9 @@\n     \r\n     \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n     \r\n     text = page.get_text()  # Estrazione del testo da una pagina\r\n-    doc = nlp_model(text)   # Processamento del testo con il modello spaCy\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n     return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n \r\n def create_text_units(sentences, unit_size=3):\r\n     \r\n@@ -75,19 +75,19 @@\n     return units\r\n \r\n def process_pdf(pdf_path, nlp_model):\r\n     \r\n-    \"\"\"Processamento del PDF e restituisce tutte le frasi.\"\"\"\r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n     \r\n-    document = fitz.open(pdf_path)  # Apri il PDF\r\n+    document = fitz.open(pdf_path)  # Aprtura del PDF\r\n     all_sentences = []\r\n \r\n-    for page_num in range(document.page_count):  # Itera su ogni pagina del PDF\r\n+    for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n-        sentences = extract_sentences_from_page(page, nlp_model)  # Estrai le frasi da questa pagina\r\n-        all_sentences.extend(sentences)  # Aggiungi le frasi all'elenco totale\r\n+        sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n+        all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n \r\n-    document.close()  # Chiudi il PDF\r\n+    document.close()  # Chiusura del PDF\r\n     return all_sentences\r\n \r\n @app.route(route=\"http_trigger_chunking\")\r\n def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n@@ -95,9 +95,9 @@\n     \r\n     # Percorso del PDF da analizzare\r\n     pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n     \r\n-    # Estrazione delle frasi a partire dal PDF\r\n+    # Creazione delle unità di testo a partire dal PDF\r\n     sentences = process_pdf(pdf_path, nlp_en)\r\n     \r\n     # Iterazione su tutte le frasi estrette e log di ogni frase\r\n     for sent in sentences:\r\n"
                },
                {
                    "date": 1729064561962,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,10 +78,14 @@\n     \r\n     \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n     \r\n     document = fitz.open(pdf_path)  # Aprtura del PDF\r\n-    all_sentences = []\r\n+    all_units = []\r\n \r\n+    with ProcessPoolExecutor() as executor:\r\n+        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n+        sentences_per_page = list(executor.map(extract_sentences_from_page))\r\n+\r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n         all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n"
                },
                {
                    "date": 1729064608001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,152 @@\n+import azure.functions as func\r\n+import logging\r\n+import os\r\n+import fitz # PyMuPDF\r\n+import spacy\r\n+import redis\r\n+from concurrent.futures import ProcessPoolExecutor\r\n+\r\n+# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n+REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n+REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n+REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n+\r\n+# Configurazione del logger di Windows Server\r\n+def configure_logger():\r\n+    logger = logging.getLogger(\"Chunking\")\r\n+    if not logger.handlers:  # Evita duplicati\r\n+        logger.setLevel(logging.INFO)\r\n+\r\n+        log_directory = './scraping_logs'\r\n+        os.makedirs(log_directory, exist_ok=True)\r\n+\r\n+        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n+        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n+\r\n+        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n+        info_handler.setFormatter(formatter)\r\n+        error_handler.setFormatter(formatter)\r\n+\r\n+        info_handler.setLevel(logging.INFO)\r\n+        error_handler.setLevel(logging.ERROR)\r\n+\r\n+        console_handler = logging.StreamHandler()\r\n+        console_handler.setFormatter(formatter)\r\n+\r\n+        logger.addHandler(info_handler)\r\n+        logger.addHandler(error_handler)\r\n+        logger.addHandler(console_handler)\r\n+\r\n+    return logger\r\n+\r\n+app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n+\r\n+# Inizializzazione del logger\r\n+logger = configure_logger()\r\n+\r\n+# Caricamento del modello per l'italiano\r\n+nlp_it = spacy.load('it_core_news_sm')\r\n+# Caricamento del modello per l'inglese\r\n+nlp_en = spacy.load('en_core_web_sm')\r\n+\r\n+\r\n+def extract_sentences_from_page(page, nlp_model):\r\n+    \r\n+    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n+    \r\n+    text = page.get_text()  # Estrazione del testo da una pagina\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n+    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n+\r\n+def create_text_units(sentences, unit_size=3):\r\n+    \r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    \r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size])\r\n+            units.append(unit)\r\n+            buffer.pop(0)\r\n+    \r\n+    return units\r\n+\r\n+def process_pdf(pdf_path, nlp_model):\r\n+    \r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n+    \r\n+    document = fitz.open(pdf_path)  # Aprtura del PDF\r\n+    all_units = []\r\n+\r\n+    with ProcessPoolExecutor() as executor:\r\n+        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n+        sentences_per_page = list(executor.map(extract_sentences_from_page, document))\r\n+\r\n+    for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n+        page = document[page_num]\r\n+        sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n+        all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n+\r\n+    document.close()  # Chiusura del PDF\r\n+    return all_sentences\r\n+\r\n+@app.route(route=\"http_trigger_chunking\")\r\n+def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n+    logger.info('Python HTTP trigger function processed a request.')\r\n+    \r\n+    # Percorso del PDF da analizzare\r\n+    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n+    \r\n+    # Creazione delle unità di testo a partire dal PDF\r\n+    sentences = process_pdf(pdf_path, nlp_en)\r\n+    \r\n+    # Iterazione su tutte le frasi estrette e log di ogni frase\r\n+    for sent in sentences:\r\n+        logger.info(sent)\r\n+    \r\n+    # Connessione a Redis\r\n+    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n+    \r\n+    # Documentazione di Red Hat 8\r\n+    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n+    # Documentazione di Red Hat 9\r\n+    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n+    # Documentazione di Windows Server\r\n+    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n+    \r\n+    # Lista dei PDF Red Hat 8\r\n+    pdf_relh8_files = []\r\n+    # Lista dei PDF Red Hat 9\r\n+    pdf_relh9_files = []\r\n+    # Lista dei PDF Windows Server\r\n+    pdf_ws_files = []\r\n+    \r\n+    # Scansione della documentazione di Red Hat 8\r\n+    for filename in os.listdir(directory_relh8_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh8_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Red Hat 9\r\n+    for filename in os.listdir(directory_relh9_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh9_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Windows Server\r\n+    for filename in os.listdir(directory_ws_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_ws_files.append(filename)\r\n+    \r\n+    # Test della connessione\r\n+    try:\r\n+        \r\n+        if client.ping():\r\n+            logger.info(\"Connessione a Redis riuscita!\")\r\n+            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n+        \r\n+    except Exception as e:\r\n+        \r\n+        logger.error(f\"Errore di connessione: {e}\")\r\n+        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729064647157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,163 +81,15 @@\n     document = fitz.open(pdf_path)  # Aprtura del PDF\r\n     all_units = []\r\n \r\n     with ProcessPoolExecutor() as executor:\r\n+        \r\n         # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n         sentences_per_page = list(executor.map(extract_sentences_from_page, document))\r\n-\r\n-    for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n-        page = document[page_num]\r\n-        sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n-        all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n-\r\n-    document.close()  # Chiusura del PDF\r\n-    return all_sentences\r\n-\r\n-@app.route(route=\"http_trigger_chunking\")\r\n-def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n-    logger.info('Python HTTP trigger function processed a request.')\r\n-    \r\n-    # Percorso del PDF da analizzare\r\n-    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n-    \r\n-    # Creazione delle unità di testo a partire dal PDF\r\n-    sentences = process_pdf(pdf_path, nlp_en)\r\n-    \r\n-    # Iterazione su tutte le frasi estrette e log di ogni frase\r\n-    for sent in sentences:\r\n-        logger.info(sent)\r\n-    \r\n-    # Connessione a Redis\r\n-    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n-    \r\n-    # Documentazione di Red Hat 8\r\n-    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n-    # Documentazione di Red Hat 9\r\n-    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n-    # Documentazione di Windows Server\r\n-    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n-    \r\n-    # Lista dei PDF Red Hat 8\r\n-    pdf_relh8_files = []\r\n-    # Lista dei PDF Red Hat 9\r\n-    pdf_relh9_files = []\r\n-    # Lista dei PDF Windows Server\r\n-    pdf_ws_files = []\r\n-    \r\n-    # Scansione della documentazione di Red Hat 8\r\n-    for filename in os.listdir(directory_relh8_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh8_files.append(filename)\r\n-    \r\n-    # Scansione della documentazione di Red Hat 9\r\n-    for filename in os.listdir(directory_relh9_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh9_files.append(filename)\r\n-    \r\n-    # Scansione della documentazione di Windows Server\r\n-    for filename in os.listdir(directory_ws_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_ws_files.append(filename)\r\n-    \r\n-    # Test della connessione\r\n-    try:\r\n         \r\n-        if client.ping():\r\n-            logger.info(\"Connessione a Redis riuscita!\")\r\n-            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n-        \r\n-    except Exception as e:\r\n-        \r\n-        logger.error(f\"Errore di connessione: {e}\")\r\n-        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n-import azure.functions as func\r\n-import logging\r\n-import os\r\n-import fitz # PyMuPDF\r\n-import spacy\r\n-import redis\r\n-from concurrent.futures import ProcessPoolExecutor\r\n+        # Unione delle frase estratte in un'unica lista\r\n+        all_sentences = \r\n \r\n-# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n-REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n-REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n-REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n-\r\n-# Configurazione del logger di Windows Server\r\n-def configure_logger():\r\n-    logger = logging.getLogger(\"Chunking\")\r\n-    if not logger.handlers:  # Evita duplicati\r\n-        logger.setLevel(logging.INFO)\r\n-\r\n-        log_directory = './scraping_logs'\r\n-        os.makedirs(log_directory, exist_ok=True)\r\n-\r\n-        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n-        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n-\r\n-        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n-        info_handler.setFormatter(formatter)\r\n-        error_handler.setFormatter(formatter)\r\n-\r\n-        info_handler.setLevel(logging.INFO)\r\n-        error_handler.setLevel(logging.ERROR)\r\n-\r\n-        console_handler = logging.StreamHandler()\r\n-        console_handler.setFormatter(formatter)\r\n-\r\n-        logger.addHandler(info_handler)\r\n-        logger.addHandler(error_handler)\r\n-        logger.addHandler(console_handler)\r\n-\r\n-    return logger\r\n-\r\n-app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n-\r\n-# Inizializzazione del logger\r\n-logger = configure_logger()\r\n-\r\n-# Caricamento del modello per l'italiano\r\n-nlp_it = spacy.load('it_core_news_sm')\r\n-# Caricamento del modello per l'inglese\r\n-nlp_en = spacy.load('en_core_web_sm')\r\n-\r\n-\r\n-def extract_sentences_from_page(page, nlp_model):\r\n-    \r\n-    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n-    \r\n-    text = page.get_text()  # Estrazione del testo da una pagina\r\n-    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n-    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n-\r\n-def create_text_units(sentences, unit_size=3):\r\n-    \r\n-    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n-    \r\n-    units = []\r\n-    buffer = []\r\n-    \r\n-    for sentence in sentences:\r\n-        buffer.append(sentence)\r\n-        if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size])\r\n-            units.append(unit)\r\n-            buffer.pop(0)\r\n-    \r\n-    return units\r\n-\r\n-def process_pdf(pdf_path, nlp_model):\r\n-    \r\n-    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n-    \r\n-    document = fitz.open(pdf_path)  # Aprtura del PDF\r\n-    all_units = []\r\n-\r\n-    with ProcessPoolExecutor() as executor:\r\n-        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n-        sentences_per_page = list(executor.map(extract_sentences_from_page))\r\n-\r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n         all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n"
                },
                {
                    "date": 1729064667813,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,9 +86,10 @@\n         # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n         sentences_per_page = list(executor.map(extract_sentences_from_page, document))\r\n         \r\n         # Unione delle frase estratte in un'unica lista\r\n-        all_sentences = \r\n+        all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n+        \r\n \r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n"
                },
                {
                    "date": 1729064694131,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,8 +88,10 @@\n         \r\n         # Unione delle frase estratte in un'unica lista\r\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n         \r\n+        # Creazione delle unità di testo\r\n+        all_units = create_text_units(all_sentences)\r\n \r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n"
                },
                {
                    "date": 1729064723280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,8 +90,10 @@\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n         \r\n         # Creazione delle unità di testo\r\n         all_units = create_text_units(all_sentences)\r\n+    \r\n+    \r\n \r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n"
                },
                {
                    "date": 1729064752061,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,10 @@\n         \r\n         # Creazione delle unità di testo\r\n         all_units = create_text_units(all_sentences)\r\n     \r\n-    \r\n+    document.close() # Chiusura del documento\r\n+    return all_units\r\n \r\n     for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n         page = document[page_num]\r\n         sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n"
                },
                {
                    "date": 1729064760414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,16 +94,8 @@\n     \r\n     document.close() # Chiusura del documento\r\n     return all_units\r\n \r\n-    for page_num in range(document.page_count):  # Iterazione su ogni pagina del PDF\r\n-        page = document[page_num]\r\n-        sentences = extract_sentences_from_page(page, nlp_model)  # Estrazione delle frasi dalla pagina\r\n-        all_sentences.extend(sentences)  # Aggiunta delle frasi all'elenco totale\r\n-\r\n-    document.close()  # Chiusura del PDF\r\n-    return all_sentences\r\n-\r\n @app.route(route=\"http_trigger_chunking\")\r\n def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n     logger.info('Python HTTP trigger function processed a request.')\r\n     \r\n"
                },
                {
                    "date": 1729064781767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n     # Percorso del PDF da analizzare\r\n     pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n     \r\n     # Creazione delle unità di testo a partire dal PDF\r\n-    sentences = process_pdf(pdf_path, nlp_en)\r\n+    text_units = process_pdf(pdf_path, nlp_en)\r\n     \r\n     # Iterazione su tutte le frasi estrette e log di ogni frase\r\n     for sent in sentences:\r\n         logger.info(sent)\r\n"
                },
                {
                    "date": 1729064856686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,11 +104,11 @@\n     \r\n     # Creazione delle unità di testo a partire dal PDF\r\n     text_units = process_pdf(pdf_path, nlp_en)\r\n     \r\n-    # Iterazione su tutte le frasi estrette e log di ogni frase\r\n-    for sent in sentences:\r\n-        logger.info(sent)\r\n+    # Stampa delle unità di testo create\r\n+    for index, unit in enumerate(text_units):\r\n+        logger.info(f\"Unità {index+1}: {unit}\")\r\n     \r\n     # Connessione a Redis\r\n     client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n     \r\n"
                },
                {
                    "date": 1729066134430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,8 +53,10 @@\n def extract_sentences_from_page(page, nlp_model):\r\n     \r\n     \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n     \r\n+    document = fitz.open(pdf)\r\n+    \r\n     text = page.get_text()  # Estrazione del testo da una pagina\r\n     doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n     return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n \r\n"
                },
                {
                    "date": 1729066257550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,16 +49,20 @@\n # Caricamento del modello per l'inglese\r\n nlp_en = spacy.load('en_core_web_sm')\r\n \r\n \r\n-def extract_sentences_from_page(page, nlp_model):\r\n+def extract_sentences_from_page(page_number, pdf_path, nlp_model):\r\n     \r\n     \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n     \r\n-    document = fitz.open(pdf)\r\n+    document = fitz.open(pdf_path) # Apertura del PDF all'interno di ciascun processo\r\n     \r\n+    page = document.load_page(page_number) # Estrazione della pagina dal PDF\r\n     text = page.get_text()  # Estrazione del testo da una pagina\r\n     doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n+    \r\n+    document.close() # Chiusura del documento\r\n+    \r\n     return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n \r\n def create_text_units(sentences, unit_size=3):\r\n     \r\n"
                },
                {
                    "date": 1729066522134,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n     \r\n     for sentence in sentences:\r\n         buffer.append(sentence)\r\n         if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size])\r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime \r\n             units.append(unit)\r\n             buffer.pop(0)\r\n     \r\n     return units\r\n"
                },
                {
                    "date": 1729066659236,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,19 +73,22 @@\n     \r\n     for sentence in sentences:\r\n         buffer.append(sentence)\r\n         if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime \r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n             units.append(unit)\r\n-            buffer.pop(0)\r\n+            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n     \r\n     return units\r\n \r\n def process_pdf(pdf_path, nlp_model):\r\n     \r\n     \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n     \r\n-    document = fitz.open(pdf_path)  # Aprtura del PDF\r\n+    document = fitz.open(pdf_path)  # Apertura del PDF per ottenere il numero delle pagine\r\n+    num_pages = document.page_count\r\n+    document.close() # Chiusura del documento una volta ottenuto il numero delle pagine\r\n+    \r\n     all_units = []\r\n \r\n     with ProcessPoolExecutor() as executor:\r\n         \r\n"
                },
                {
                    "date": 1729066734688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,164 @@\n+import azure.functions as func\r\n+import logging\r\n+import os\r\n+import fitz # PyMuPDF\r\n+import spacy\r\n+import redis\r\n+from concurrent.futures import ProcessPoolExecutor\r\n+\r\n+# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n+REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n+REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n+REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n+\r\n+# Configurazione del logger di Windows Server\r\n+def configure_logger():\r\n+    logger = logging.getLogger(\"Chunking\")\r\n+    if not logger.handlers:  # Evita duplicati\r\n+        logger.setLevel(logging.INFO)\r\n+\r\n+        log_directory = './scraping_logs'\r\n+        os.makedirs(log_directory, exist_ok=True)\r\n+\r\n+        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n+        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n+\r\n+        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n+        info_handler.setFormatter(formatter)\r\n+        error_handler.setFormatter(formatter)\r\n+\r\n+        info_handler.setLevel(logging.INFO)\r\n+        error_handler.setLevel(logging.ERROR)\r\n+\r\n+        console_handler = logging.StreamHandler()\r\n+        console_handler.setFormatter(formatter)\r\n+\r\n+        logger.addHandler(info_handler)\r\n+        logger.addHandler(error_handler)\r\n+        logger.addHandler(console_handler)\r\n+\r\n+    return logger\r\n+\r\n+app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n+\r\n+# Inizializzazione del logger\r\n+logger = configure_logger()\r\n+\r\n+# Caricamento del modello per l'italiano\r\n+nlp_it = spacy.load('it_core_news_sm')\r\n+# Caricamento del modello per l'inglese\r\n+nlp_en = spacy.load('en_core_web_sm')\r\n+\r\n+\r\n+def extract_sentences_from_page(page_number, pdf_path, nlp_model):\r\n+    \r\n+    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n+    \r\n+    document = fitz.open(pdf_path) # Apertura del PDF all'interno di ciascun processo\r\n+    \r\n+    page = document.load_page(page_number) # Estrazione della pagina dal PDF\r\n+    text = page.get_text()  # Estrazione del testo da una pagina\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n+    \r\n+    document.close() # Chiusura del documento\r\n+    \r\n+    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n+\r\n+def create_text_units(sentences, unit_size=3):\r\n+    \r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    \r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n+            units.append(unit)\r\n+            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n+    \r\n+    return units\r\n+\r\n+def process_pdf(pdf_path, nlp_model):\r\n+    \r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n+    \r\n+    document = fitz.open(pdf_path)  # Apertura del PDF per ottenere il numero delle pagine\r\n+    num_pages = document.page_count\r\n+    document.close() # Chiusura del documento una volta ottenuto il numero delle pagine\r\n+    \r\n+    all_units = []\r\n+\r\n+    with ProcessPoolExecutor() as executor:\r\n+        \r\n+        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n+        sentences_per_page = list(executor.map(\r\n+            lambda page_num: extract_sentences_from_page(page_num, pdf_path,), document))\r\n+        \r\n+        # Unione delle frase estratte in un'unica lista\r\n+        all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n+        \r\n+        # Creazione delle unità di testo\r\n+        all_units = create_text_units(all_sentences)\r\n+    \r\n+    document.close() # Chiusura del documento\r\n+    return all_units\r\n+\r\n+@app.route(route=\"http_trigger_chunking\")\r\n+def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n+    logger.info('Python HTTP trigger function processed a request.')\r\n+    \r\n+    # Percorso del PDF da analizzare\r\n+    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n+    \r\n+    # Creazione delle unità di testo a partire dal PDF\r\n+    text_units = process_pdf(pdf_path, nlp_en)\r\n+    \r\n+    # Stampa delle unità di testo create\r\n+    for index, unit in enumerate(text_units):\r\n+        logger.info(f\"Unità {index+1}: {unit}\")\r\n+    \r\n+    # Connessione a Redis\r\n+    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n+    \r\n+    # Documentazione di Red Hat 8\r\n+    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n+    # Documentazione di Red Hat 9\r\n+    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n+    # Documentazione di Windows Server\r\n+    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n+    \r\n+    # Lista dei PDF Red Hat 8\r\n+    pdf_relh8_files = []\r\n+    # Lista dei PDF Red Hat 9\r\n+    pdf_relh9_files = []\r\n+    # Lista dei PDF Windows Server\r\n+    pdf_ws_files = []\r\n+    \r\n+    # Scansione della documentazione di Red Hat 8\r\n+    for filename in os.listdir(directory_relh8_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh8_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Red Hat 9\r\n+    for filename in os.listdir(directory_relh9_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh9_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Windows Server\r\n+    for filename in os.listdir(directory_ws_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_ws_files.append(filename)\r\n+    \r\n+    # Test della connessione\r\n+    try:\r\n+        \r\n+        if client.ping():\r\n+            logger.info(\"Connessione a Redis riuscita!\")\r\n+            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n+        \r\n+    except Exception as e:\r\n+        \r\n+        logger.error(f\"Errore di connessione: {e}\")\r\n+        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729066746831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,9 +93,9 @@\n     with ProcessPoolExecutor() as executor:\r\n         \r\n         # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n         sentences_per_page = list(executor.map(\r\n-            lambda page_num: extract_sentences_from_page(page_num, pdf_path,), document))\r\n+            lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), document))\r\n         \r\n         # Unione delle frase estratte in un'unica lista\r\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n         \r\n@@ -160,168 +160,5 @@\n         \r\n     except Exception as e:\r\n         \r\n         logger.error(f\"Errore di connessione: {e}\")\r\n-        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n-import azure.functions as func\r\n-import logging\r\n-import os\r\n-import fitz # PyMuPDF\r\n-import spacy\r\n-import redis\r\n-from concurrent.futures import ProcessPoolExecutor\r\n-\r\n-# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n-REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n-REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n-REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n-\r\n-# Configurazione del logger di Windows Server\r\n-def configure_logger():\r\n-    logger = logging.getLogger(\"Chunking\")\r\n-    if not logger.handlers:  # Evita duplicati\r\n-        logger.setLevel(logging.INFO)\r\n-\r\n-        log_directory = './scraping_logs'\r\n-        os.makedirs(log_directory, exist_ok=True)\r\n-\r\n-        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n-        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n-\r\n-        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n-        info_handler.setFormatter(formatter)\r\n-        error_handler.setFormatter(formatter)\r\n-\r\n-        info_handler.setLevel(logging.INFO)\r\n-        error_handler.setLevel(logging.ERROR)\r\n-\r\n-        console_handler = logging.StreamHandler()\r\n-        console_handler.setFormatter(formatter)\r\n-\r\n-        logger.addHandler(info_handler)\r\n-        logger.addHandler(error_handler)\r\n-        logger.addHandler(console_handler)\r\n-\r\n-    return logger\r\n-\r\n-app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n-\r\n-# Inizializzazione del logger\r\n-logger = configure_logger()\r\n-\r\n-# Caricamento del modello per l'italiano\r\n-nlp_it = spacy.load('it_core_news_sm')\r\n-# Caricamento del modello per l'inglese\r\n-nlp_en = spacy.load('en_core_web_sm')\r\n-\r\n-\r\n-def extract_sentences_from_page(page_number, pdf_path, nlp_model):\r\n-    \r\n-    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n-    \r\n-    document = fitz.open(pdf_path) # Apertura del PDF all'interno di ciascun processo\r\n-    \r\n-    page = document.load_page(page_number) # Estrazione della pagina dal PDF\r\n-    text = page.get_text()  # Estrazione del testo da una pagina\r\n-    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n-    \r\n-    document.close() # Chiusura del documento\r\n-    \r\n-    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n-\r\n-def create_text_units(sentences, unit_size=3):\r\n-    \r\n-    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n-    \r\n-    units = []\r\n-    buffer = []\r\n-    \r\n-    for sentence in sentences:\r\n-        buffer.append(sentence)\r\n-        if len(buffer) >= unit_size:\r\n-            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n-            units.append(unit)\r\n-            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n-    \r\n-    return units\r\n-\r\n-def process_pdf(pdf_path, nlp_model):\r\n-    \r\n-    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n-    \r\n-    document = fitz.open(pdf_path)  # Apertura del PDF per ottenere il numero delle pagine\r\n-    num_pages = document.page_count\r\n-    document.close() # Chiusura del documento una volta ottenuto il numero delle pagine\r\n-    \r\n-    all_units = []\r\n-\r\n-    with ProcessPoolExecutor() as executor:\r\n-        \r\n-        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n-        sentences_per_page = list(executor.map(extract_sentences_from_page, document))\r\n-        \r\n-        # Unione delle frase estratte in un'unica lista\r\n-        all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n-        \r\n-        # Creazione delle unità di testo\r\n-        all_units = create_text_units(all_sentences)\r\n-    \r\n-    document.close() # Chiusura del documento\r\n-    return all_units\r\n-\r\n-@app.route(route=\"http_trigger_chunking\")\r\n-def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n-    logger.info('Python HTTP trigger function processed a request.')\r\n-    \r\n-    # Percorso del PDF da analizzare\r\n-    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n-    \r\n-    # Creazione delle unità di testo a partire dal PDF\r\n-    text_units = process_pdf(pdf_path, nlp_en)\r\n-    \r\n-    # Stampa delle unità di testo create\r\n-    for index, unit in enumerate(text_units):\r\n-        logger.info(f\"Unità {index+1}: {unit}\")\r\n-    \r\n-    # Connessione a Redis\r\n-    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n-    \r\n-    # Documentazione di Red Hat 8\r\n-    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n-    # Documentazione di Red Hat 9\r\n-    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n-    # Documentazione di Windows Server\r\n-    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n-    \r\n-    # Lista dei PDF Red Hat 8\r\n-    pdf_relh8_files = []\r\n-    # Lista dei PDF Red Hat 9\r\n-    pdf_relh9_files = []\r\n-    # Lista dei PDF Windows Server\r\n-    pdf_ws_files = []\r\n-    \r\n-    # Scansione della documentazione di Red Hat 8\r\n-    for filename in os.listdir(directory_relh8_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh8_files.append(filename)\r\n-    \r\n-    # Scansione della documentazione di Red Hat 9\r\n-    for filename in os.listdir(directory_relh9_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_relh9_files.append(filename)\r\n-    \r\n-    # Scansione della documentazione di Windows Server\r\n-    for filename in os.listdir(directory_ws_path):\r\n-        if filename.endswith('.pdf'):\r\n-            pdf_ws_files.append(filename)\r\n-    \r\n-    # Test della connessione\r\n-    try:\r\n-        \r\n-        if client.ping():\r\n-            logger.info(\"Connessione a Redis riuscita!\")\r\n-            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n-        \r\n-    except Exception as e:\r\n-        \r\n-        logger.error(f\"Errore di connessione: {e}\")\r\n         return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1729066781444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,10 +93,11 @@\n     with ProcessPoolExecutor() as executor:\r\n         \r\n         # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n         sentences_per_page = list(executor.map(\r\n-            lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), document))\r\n-        \r\n+            lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), \r\n+            range(num_pages)\r\n+        ))\r\n         # Unione delle frase estratte in un'unica lista\r\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n         \r\n         # Creazione delle unità di testo\r\n"
                },
                {
                    "date": 1729066787928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,8 +96,9 @@\n         sentences_per_page = list(executor.map(\r\n             lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), \r\n             range(num_pages)\r\n         ))\r\n+        \r\n         # Unione delle frase estratte in un'unica lista\r\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n         \r\n         # Creazione delle unità di testo\r\n"
                },
                {
                    "date": 1729066848271,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,9 @@\n     all_units = []\r\n \r\n     with ProcessPoolExecutor() as executor:\r\n         \r\n-        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo\r\n+        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo: ciascun processo riceve il nuemro della pagina e il percorso del PDF\r\n         sentences_per_page = list(executor.map(\r\n             lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), \r\n             range(num_pages)\r\n         ))\r\n@@ -103,9 +103,8 @@\n         \r\n         # Creazione delle unità di testo\r\n         all_units = create_text_units(all_sentences)\r\n     \r\n-    document.close() # Chiusura del documento\r\n     return all_units\r\n \r\n @app.route(route=\"http_trigger_chunking\")\r\n def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n"
                },
                {
                    "date": 1729067319530,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,10 +93,10 @@\n     with ProcessPoolExecutor() as executor:\r\n         \r\n         # Estrazione delle frasi da ciascuna pagina del PDF in parallelo: ciascun processo riceve il nuemro della pagina e il percorso del PDF\r\n         sentences_per_page = list(executor.map(\r\n-            lambda page_num: extract_sentences_from_page(page_num, pdf_path, nlp_model), \r\n-            range(num_pages)\r\n+            extract_sentences_from_page, \r\n+            range(num_pages),\r\n         ))\r\n         \r\n         # Unione delle frase estratte in un'unica lista\r\n         all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n"
                },
                {
                    "date": 1729067431675,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,167 @@\n+import azure.functions as func\r\n+import logging\r\n+import os\r\n+import fitz # PyMuPDF\r\n+import spacy\r\n+import redis\r\n+from concurrent.futures import ProcessPoolExecutor\r\n+\r\n+# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\n+REDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\n+REDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\n+REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n+\r\n+# Configurazione del logger di Windows Server\r\n+def configure_logger():\r\n+    logger = logging.getLogger(\"Chunking\")\r\n+    if not logger.handlers:  # Evita duplicati\r\n+        logger.setLevel(logging.INFO)\r\n+\r\n+        log_directory = './scraping_logs'\r\n+        os.makedirs(log_directory, exist_ok=True)\r\n+\r\n+        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n+        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n+\r\n+        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n+        info_handler.setFormatter(formatter)\r\n+        error_handler.setFormatter(formatter)\r\n+\r\n+        info_handler.setLevel(logging.INFO)\r\n+        error_handler.setLevel(logging.ERROR)\r\n+\r\n+        console_handler = logging.StreamHandler()\r\n+        console_handler.setFormatter(formatter)\r\n+\r\n+        logger.addHandler(info_handler)\r\n+        logger.addHandler(error_handler)\r\n+        logger.addHandler(console_handler)\r\n+\r\n+    return logger\r\n+\r\n+app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n+\r\n+# Inizializzazione del logger\r\n+logger = configure_logger()\r\n+\r\n+# Caricamento del modello per l'italiano\r\n+nlp_it = spacy.load('it_core_news_sm')\r\n+# Caricamento del modello per l'inglese\r\n+nlp_en = spacy.load('en_core_web_sm')\r\n+\r\n+\r\n+def extract_sentences_from_page(page_number, pdf_path, nlp_model):\r\n+    \r\n+    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n+    \r\n+    document = fitz.open(pdf_path) # Apertura del PDF all'interno di ciascun processo\r\n+    \r\n+    page = document.load_page(page_number) # Estrazione della pagina dal PDF\r\n+    text = page.get_text()  # Estrazione del testo da una pagina\r\n+    doc = nlp_model(text)   # Elaborazione del testo con il modello spaCy\r\n+    \r\n+    document.close() # Chiusura del documento\r\n+    \r\n+    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n+\r\n+def create_text_units(sentences, unit_size=3):\r\n+    \r\n+    \"\"\"Creazione delle unità di testo costituite da `unit_size` frasi contigue.\"\"\"\r\n+    \r\n+    units = []\r\n+    buffer = []\r\n+    \r\n+    for sentence in sentences:\r\n+        buffer.append(sentence)\r\n+        if len(buffer) >= unit_size:\r\n+            unit = \" \".join(buffer[:unit_size]) # Creazione dell'unità unendo le prime `unit_size` frasi del buffer\r\n+            units.append(unit)\r\n+            buffer.pop(0) # Rimozione della prima frase dal buffet (shift)\r\n+    \r\n+    return units\r\n+\r\n+def process_pdf(pdf_path, nlp_model):\r\n+    \r\n+    \"\"\"Elaborazione del PDF e restituzione di tutte le frasi.\"\"\"\r\n+    \r\n+    document = fitz.open(pdf_path)  # Apertura del PDF per ottenere il numero delle pagine\r\n+    num_pages = document.page_count\r\n+    document.close() # Chiusura del documento una volta ottenuto il numero delle pagine\r\n+    \r\n+    all_units = []\r\n+\r\n+    with ProcessPoolExecutor() as executor:\r\n+        \r\n+        # Estrazione delle frasi da ciascuna pagina del PDF in parallelo: ciascun processo riceve il nuemro della pagina e il percorso del PDF\r\n+        sentences_per_page = list(executor.map(\r\n+            extract_sentences_from_page, \r\n+            range(num_pages),\r\n+            [pdf_path] * num_pages,  # Passaggio del percorso PDF a ogni processo\r\n+            [nlp_model] * num_pages  # Passaggio del modello NLP a ogni processo\r\n+        ))\r\n+        \r\n+        # Unione delle frase estratte in un'unica lista\r\n+        all_sentences = [sentence for page_sentences in sentences_per_page for sentence in page_sentences]\r\n+        \r\n+        # Creazione delle unità di testo\r\n+        all_units = create_text_units(all_sentences)\r\n+    \r\n+    return all_units\r\n+\r\n+@app.route(route=\"http_trigger_chunking\")\r\n+def http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n+    logger.info('Python HTTP trigger function processed a request.')\r\n+    \r\n+    # Percorso del PDF da analizzare\r\n+    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n+    \r\n+    # Creazione delle unità di testo a partire dal PDF\r\n+    text_units = process_pdf(pdf_path, nlp_en)\r\n+    \r\n+    # Stampa delle unità di testo create\r\n+    for index, unit in enumerate(text_units):\r\n+        logger.info(f\"Unità {index+1}: {unit}\")\r\n+    \r\n+    # Connessione a Redis\r\n+    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n+    \r\n+    # Documentazione di Red Hat 8\r\n+    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n+    # Documentazione di Red Hat 9\r\n+    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n+    # Documentazione di Windows Server\r\n+    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n+    \r\n+    # Lista dei PDF Red Hat 8\r\n+    pdf_relh8_files = []\r\n+    # Lista dei PDF Red Hat 9\r\n+    pdf_relh9_files = []\r\n+    # Lista dei PDF Windows Server\r\n+    pdf_ws_files = []\r\n+    \r\n+    # Scansione della documentazione di Red Hat 8\r\n+    for filename in os.listdir(directory_relh8_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh8_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Red Hat 9\r\n+    for filename in os.listdir(directory_relh9_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_relh9_files.append(filename)\r\n+    \r\n+    # Scansione della documentazione di Windows Server\r\n+    for filename in os.listdir(directory_ws_path):\r\n+        if filename.endswith('.pdf'):\r\n+            pdf_ws_files.append(filename)\r\n+    \r\n+    # Test della connessione\r\n+    try:\r\n+        \r\n+        if client.ping():\r\n+            logger.info(\"Connessione a Redis riuscita!\")\r\n+            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n+        \r\n+    except Exception as e:\r\n+        \r\n+        logger.error(f\"Errore di connessione: {e}\")\r\n+        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)\n\\ No newline at end of file\n"
                }
            ],
            "date": 1729063782966,
            "name": "Commit-0",
            "content": "import azure.functions as func\r\nimport logging\r\nimport os\r\nimport fitz # PyMuPDF\r\nimport spacy\r\nimport redis\r\nfrom concurrent.futures import ProcessPoolExecutor\r\n\r\n# Configurazione per connettersi a un'istanza di Azure Cache for Redis\r\nREDIS_HOST = os.getenv('REDIS_HOST', 'metis.redis.cache.windows.net')\r\nREDIS_PORT = int(os.getenv('REDIS_PORT', 6380))  # Porta SSL\r\nREDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'S1DHsgrmOCZSCaGw5tW9Yh01bg64v9g7YAzCaFEbFsA=')  # Primary Key\r\n\r\n# Configurazione del logger di Windows Server\r\ndef configure_logger():\r\n    logger = logging.getLogger(\"Chunking\")\r\n    if not logger.handlers:  # Evita duplicati\r\n        logger.setLevel(logging.INFO)\r\n\r\n        log_directory = './scraping_logs'\r\n        os.makedirs(log_directory, exist_ok=True)\r\n\r\n        info_handler = logging.FileHandler(os.path.join(log_directory, 'scraping.log'), encoding='utf-8')\r\n        error_handler = logging.FileHandler(os.path.join(log_directory, 'error.log'), encoding='utf-8')\r\n\r\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n        info_handler.setFormatter(formatter)\r\n        error_handler.setFormatter(formatter)\r\n\r\n        info_handler.setLevel(logging.INFO)\r\n        error_handler.setLevel(logging.ERROR)\r\n\r\n        console_handler = logging.StreamHandler()\r\n        console_handler.setFormatter(formatter)\r\n\r\n        logger.addHandler(info_handler)\r\n        logger.addHandler(error_handler)\r\n        logger.addHandler(console_handler)\r\n\r\n    return logger\r\n\r\napp = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)\r\n\r\n# Inizializzazione del logger\r\nlogger = configure_logger()\r\n\r\n# Caricamento del modello per l'italiano\r\nnlp_it = spacy.load('it_core_news_sm')\r\n# Caricamento del modello per l'inglese\r\nnlp_en = spacy.load('en_core_web_sm')\r\n\r\n\r\ndef extract_sentences_from_page(page, nlp_model):\r\n    \"\"\"Estrazione delle frasi da una singola pagina del PDF usando spaCy.\"\"\"\r\n    text = page.get_text()  # Estrazione del testo da una pagina\r\n    doc = nlp_model(text)   # Processamento del testo con il modello spaCy\r\n    return [sent.text for sent in doc.sents]  # Ritorno delle frasi processate\r\n\r\ndef create_text_units(sentences, unit_size=3):\r\n    \"\"\"Creazione delle unità di testo costituite da \"\"\"\r\n\r\ndef process_pdf(pdf_path, nlp_model):\r\n    \"\"\"Processa il PDF e restituisce tutte le frasi.\"\"\"\r\n    document = fitz.open(pdf_path)  # Apri il PDF\r\n    all_sentences = []\r\n\r\n    for page_num in range(document.page_count):  # Itera su ogni pagina del PDF\r\n        page = document[page_num]\r\n        sentences = extract_sentences_from_page(page, nlp_model)  # Estrai le frasi da questa pagina\r\n        all_sentences.extend(sentences)  # Aggiungi le frasi all'elenco totale\r\n\r\n    document.close()  # Chiudi il PDF\r\n    return all_sentences\r\n\r\n@app.route(route=\"http_trigger_chunking\")\r\ndef http_trigger_chunking(req: func.HttpRequest) -> func.HttpResponse:\r\n    logger.info('Python HTTP trigger function processed a request.')\r\n    \r\n    # Percorso del PDF da analizzare\r\n    pdf_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Chunking\\Red_Hat_Enterprise_Linux-8-8.0_Release_Notes-en-US.pdf\"\r\n    \r\n    # Estrazione delle frasi a partire dal PDF\r\n    sentences = process_pdf(pdf_path, nlp_en)\r\n    \r\n    # Iterazione su tutte le frasi estrette e log di ogni frase\r\n    for sent in sentences:\r\n        logger.info(sent)\r\n    \r\n    # Connessione a Redis\r\n    client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, ssl=True)\r\n    \r\n    # Documentazione di Red Hat 8\r\n    directory_relh8_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat8\\src\\functions\\documentsRelH8\"\r\n    # Documentazione di Red Hat 9\r\n    directory_relh9_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\RedHat9\\src\\functions\\documentsRelH9\"\r\n    # Documentazione di Windows Server\r\n    directory_ws_path = r\"C:\\Users\\rdell\\OneDrive - Politecnico di Torino\\Desktop\\Reply9\\METIS\\Scraping\\WindowsServer\\documentsWinServer\"\r\n    \r\n    # Lista dei PDF Red Hat 8\r\n    pdf_relh8_files = []\r\n    # Lista dei PDF Red Hat 9\r\n    pdf_relh9_files = []\r\n    # Lista dei PDF Windows Server\r\n    pdf_ws_files = []\r\n    \r\n    # Scansione della documentazione di Red Hat 8\r\n    for filename in os.listdir(directory_relh8_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_relh8_files.append(filename)\r\n    \r\n    # Scansione della documentazione di Red Hat 9\r\n    for filename in os.listdir(directory_relh9_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_relh9_files.append(filename)\r\n    \r\n    # Scansione della documentazione di Windows Server\r\n    for filename in os.listdir(directory_ws_path):\r\n        if filename.endswith('.pdf'):\r\n            pdf_ws_files.append(filename)\r\n    \r\n    # Test della connessione\r\n    try:\r\n        \r\n        if client.ping():\r\n            logger.info(\"Connessione a Redis riuscita!\")\r\n            return func.HttpResponse(\"Connessione a Redis riuscita! Ciao Raffaele, benvenuto!\", status_code=200)\r\n        \r\n    except Exception as e:\r\n        \r\n        logger.error(f\"Errore di connessione: {e}\")\r\n        return func.HttpResponse(f\"Errore di connessione: {e}\", status_code=500)"
        }
    ]
}